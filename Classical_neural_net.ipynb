{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target location of output\n",
    "folder_locator = './outputs/'\n",
    "\n",
    "# Locations of TensorBoard and model save outputs\n",
    "board_string = folder_locator + 'tensorboard/5_classic/'\n",
    "checkpoint_string = folder_locator + 'models/5_classic/'\n",
    "# Number of batches to use in the optimization\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('unsw_train.csv' , delimiter=',')\n",
    "test = np.loadtxt('unsw_test.csv' , delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape  (82300, 43)\n",
      "test shape  (60000, 43)\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape \" , train.shape)\n",
    "print(\"test shape \" ,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[: , :42]\n",
    "Y_train = train[: , 42:]\n",
    "X_test =  test[: , :42]\n",
    "Y_test =  test[: , 42:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape:  (82300, 42)\n",
      "y train shape:  (82300, 1)\n",
      "x test shape:  (60000, 42)\n",
      "y test shape:  (60000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x train shape: \",X_train.shape)\n",
    "print(\"y train shape: \",Y_train.shape)\n",
    "print(\"x test shape: \",X_test.shape)\n",
    "print(\"y test shape: \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, dimension of a training example = 42\n",
    "    n_y -- scalar, dimension of output = 1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[n_x,None], name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=[n_y,None], name='Y')    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [30, 25]\n",
    "                        b1 : [30, 1]\n",
    "                        W2 : [25, 60]\n",
    "                        b2 : [25, 1]\n",
    "                        W3 : [60, 60]\n",
    "                        b3 : [60, 1]\n",
    "                        W4 : [60,60]\n",
    "                        b4 : [60,1]\n",
    "                        W5 : [60,60]\n",
    "                        b5 : [60,1]\n",
    "                        W6 : [60,60]\n",
    "                        b6 : [60,1]\n",
    "                        W7 : [1,60]\n",
    "                        b7 : [1,1]\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)   \n",
    "        \n",
    "    W1 = tf.get_variable('W1', [25, 42], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable('b1', [25, 1], initializer=tf.zeros_initializer())\n",
    "    W2 = tf.get_variable('W2', [60, 25], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable('b2', [60, 1], initializer=tf.zeros_initializer())\n",
    "    W3 = tf.get_variable('W3', [60, 60], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable('b3', [60, 1], initializer=tf.zeros_initializer())\n",
    "    W4 = tf.get_variable('W4', [60, 60], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b4 = tf.get_variable('b4', [60, 1], initializer=tf.zeros_initializer())\n",
    "    W5 = tf.get_variable('W5', [60, 60], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b5 = tf.get_variable('b5', [60 , 1], initializer=tf.zeros_initializer())\n",
    "    W6 = tf.get_variable('W6', [60, 60], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b6 = tf.get_variable('b6', [60 , 1], initializer=tf.zeros_initializer())\n",
    "    W7 = tf.get_variable('W7', [1, 60], initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b7 = tf.get_variable('b7', [1, 1], initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5,\n",
    "                  \"W6\": W6,\n",
    "                  \"b6\": b6,\n",
    "                  \"W7\": W7,\n",
    "                  \"b7\": b7}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5']\n",
    "    W6 = parameters['W6']\n",
    "    b6 = parameters['b6']\n",
    "    W7 = parameters['W7']\n",
    "    b7 = parameters['b7']\n",
    "    X = tf.transpose(X)\n",
    "    print(W1)\n",
    "    print(X.shape)\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)\n",
    "    A1 = tf.nn.elu(Z1)\n",
    "    print(A1.shape)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)\n",
    "    A2 = tf.nn.elu(Z2)\n",
    "    print(A2.shape)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)\n",
    "    A3 = tf.nn.elu(Z3)\n",
    "    \n",
    "    Z4 = tf.add(tf.matmul(W4 , A3) , b4)\n",
    "    A4 =  tf.nn.elu(Z4)\n",
    "    \n",
    "    Z5 = tf.add(tf.matmul(W5 , A4) , b5)\n",
    "    A5 = tf.nn.elu(Z5)\n",
    "    \n",
    "    Z6 =  tf.add(tf.matmul(W6 , A5) , b6)\n",
    "    A6 = tf.nn.elu(Z6)\n",
    "    \n",
    "    Z7 = tf.add(tf.matmul(W7 , A6 ) , b7)\n",
    "    print(\"Z7 shape : \",Z7.shape)\n",
    "    return Z7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z7, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.sigmoid_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z7)\n",
    "    #labels = tf.transpose(Y)\n",
    "    labels = Y\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(z, y):\n",
    "    z_array = np.array(z).T\n",
    "    y_array = np.array(y).T\n",
    "    yhat_labels = (z_array > 0.5).astype(np.int)\n",
    "    correct_labels = np.sum(y_array == yhat_labels)\n",
    "    accuracy = correct_labels/y_array.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 3000, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 10, number of training examples = 72161)\n",
    "    Y_train -- test set, of shape (output size = 2, number of training examples = 72161)\n",
    "    X_test -- training set, of shape (input size = 10, number of training examples = 48108)\n",
    "    Y_test -- test set, of shape (output size = 2, number of test examples = 48108)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    train_acc = []                                    # To keep track of the training accuracy\n",
    "    test_acc = []                                     # To keep track of the test accuracy\n",
    "    #saver = tf.train.Saver(parameters)  \n",
    "\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    #X, Y = create_placeholders(n_x, n_y)\n",
    "    X = tf.placeholder(tf.float32, shape= [25,42] , name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32 , shape= [25,1] , name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z7 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z7, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use GradientDescentOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        #saver = tf.train.Saver(parameters)\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        split_X_train = np.split(X_train, len(X_train) / batch_size)\n",
    "        split_Y_train = np.split(Y_train , len(Y_train) / batch_size)\n",
    "        \n",
    "        split_X_test = np.split(X_test, len(X_test) / batch_size)\n",
    "        split_Y_test = np.split(Y_test, len(Y_test) / batch_size)\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            seed = seed + 1\n",
    "            \n",
    "            #offset = (epoch * batch_size) % (82300 - batch_size)\n",
    "            \n",
    "            #batch_data = X_train[offset:(offset + batch_size), :]\n",
    "            #batch_labels = Y_train[offset:(offset + batch_size), :]\n",
    "            #print(batch_data.shape)\n",
    "            \n",
    "            for batch in \n",
    "     \n",
    "            #_, c = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_data, Y: batch_labels})\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            epoch_cost = c                # Defines a cost related to an epoch\n",
    "            \n",
    "            test_batch_data = X_test[offset:(offset + batch_size), :]\n",
    "            test_batch_labels = Y_test[offset:(offset + batch_size), :]\n",
    "            \n",
    "            epoch_train_Z = sess.run(Z7, feed_dict={X: batch_data})\n",
    "            epoch_train_accuracy = calculate_accuracy(epoch_train_Z, batch_label)\n",
    "            epoch_test_Z = sess.run(Z7, feed_dict={X: test_batch_data})\n",
    "            epoch_test_accuracy = calculate_accuracy(epoch_test_Z, test_batch_labels)\n",
    "            \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                train_acc.append(epoch_train_accuracy)\n",
    "                test_acc.append(epoch_test_accuracy)\n",
    "            \n",
    "            #if(epoch % 1000 == 0):\n",
    "             #   saver.save(session, checkpoint_string + 'sess.ckpt', global_step=epoch)\n",
    "\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        # plot train vs. test accuracy over training epochs\n",
    "        plt.figure(figsize=(13,7))\n",
    "        plt.plot(np.squeeze(train_acc), color=\"red\")\n",
    "        plt.plot(np.squeeze(test_acc), color=\"blue\")\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Training Accuracy (Red) vs. Test Accuracy (Blue) Trend\")\n",
    "        plt.show()\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Accuracy\n",
    "        train_Z = sess.run(Z7, feed_dict={X: X_train})\n",
    "        train_accuracy = calculate_accuracy(train_Z, Y_train)\n",
    "        test_Z = sess.run(Z7, feed_dict={X: X_test})\n",
    "        test_accuracy = calculate_accuracy(test_Z, Y_test)\n",
    "\n",
    "        print (\"Train Accuracy:\", train_accuracy)\n",
    "        print (\"Test Accuracy:\", test_accuracy)\n",
    "        \n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(time.asctime( time.localtime(time.time()) ))\n",
    "parameters = model(X_train, Y_train, X_test, Y_test, num_epochs = 3000)\n",
    "end_time = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

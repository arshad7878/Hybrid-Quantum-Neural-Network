{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fraz/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2018 Xanadu Quantum Technologies Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Fraud detection fitting script\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, BSgate, Kgate, Sgate, Rgate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                   Hyperparameters\n",
    "# ===================================================================================\n",
    "\n",
    "# Two modes required: one for \"genuine\" transactions and one for \"fradulent\"\n",
    "mode_number = 2\n",
    "# Number of photonic quantum layers\n",
    "depth = 5\n",
    "\n",
    "# Fock basis truncation\n",
    "cutoff = 10\n",
    "# Number of batches in optimization\n",
    "reps = 1000\n",
    "\n",
    "# Label for simulation\n",
    "simulation_label = 1\n",
    "\n",
    "# Random initialization of gate parameters\n",
    "sdev_photon = 0.2\n",
    "sdev = 0.8\n",
    "\n",
    "# Variable clipping values\n",
    "disp_clip = 5\n",
    "sq_clip = 5\n",
    "kerr_clip = 1\n",
    "\n",
    "# If loading from checkpoint, previous batch number reached\n",
    "ckpt_val = 0\n",
    "\n",
    "# Number of repetitions between each output to TensorBoard\n",
    "tb_reps = 100\n",
    "# Number of repetitions between each model save\n",
    "savr_reps = 100\n",
    "\n",
    "model_string = str(simulation_label)\n",
    "\n",
    "# Target location of output\n",
    "folder_locator = './outputs/'\n",
    "\n",
    "# Locations of TensorBoard and model save outputs\n",
    "board_string = folder_locator + 'tensorboard/11/'\n",
    "checkpoint_string = folder_locator + 'models/11/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                   Loading the training data\n",
    "# ===================================================================================\n",
    "\n",
    "# Data outputted from data_processor.py\n",
    "data = np.loadtxt('unsw_train.csv' , delimiter=',')\n",
    "data_points = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of batches to use in the optimization\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82300, 43)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[: , 42:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the classical NN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                   Setting up the classical NN input\n",
    "# ===================================================================================\n",
    "\n",
    "# Input neurons\n",
    "input_neurons = 42\n",
    "# Widths of hidden layers\n",
    "nn_architecture = [55, 55]\n",
    "# Output neurons of classical part\n",
    "output_neurons = 14\n",
    "\n",
    "# Defining classical network parameters\n",
    "input_classical_layer = tf.placeholder(tf.float32, shape=[batch_size, input_neurons])\n",
    "\n",
    "layer_matrix_1 = tf.Variable(tf.random_normal(shape=[input_neurons, nn_architecture[0]]))\n",
    "offset_1 = tf.Variable(tf.random_normal(shape=[nn_architecture[0]]))\n",
    "\n",
    "layer_matrix_2 = tf.Variable(tf.random_normal(shape=[nn_architecture[0], nn_architecture[1]]))\n",
    "offset_2 = tf.Variable(tf.random_normal(shape=[nn_architecture[1]]))\n",
    "\n",
    "layer_matrix_3 = tf.Variable(tf.random_normal(shape=[nn_architecture[1], output_neurons]))\n",
    "offset_3 = tf.Variable(tf.random_normal(shape=[output_neurons]))\n",
    "\n",
    "# Creating hidden layers and output\n",
    "layer_1 = tf.nn.elu(tf.matmul(input_classical_layer, layer_matrix_1) + offset_1)\n",
    "layer_2 = tf.nn.elu(tf.matmul(layer_1, layer_matrix_2) + offset_2)\n",
    "\n",
    "output_layer = tf.nn.elu(tf.matmul(layer_2, layer_matrix_3) + offset_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining QNN parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                   Defining QNN parameters\n",
    "# ===================================================================================\n",
    "\n",
    "# Number of beamsplitters in interferometer\n",
    "bs_in_interferometer = int(1.0 * mode_number * (mode_number - 1) / 2)\n",
    "\n",
    "with tf.name_scope('variables'):\n",
    "    bs_variables = tf.Variable(tf.random_normal(shape=[depth, bs_in_interferometer, 2, 2]\n",
    "                                                , stddev=sdev))\n",
    "    phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number, 2], stddev=sdev))\n",
    "\n",
    "    sq_magnitude_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                          , stddev=sdev_photon))\n",
    "    sq_phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                      , stddev=sdev))\n",
    "    disp_magnitude_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                            , stddev=sdev_photon))\n",
    "    disp_phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                        , stddev=sdev))\n",
    "    kerr_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number], stddev=sdev_photon))\n",
    "\n",
    "parameters = [layer_matrix_1, offset_1, layer_matrix_2, offset_2, layer_matrix_3, offset_3, bs_variables,\n",
    "              phase_variables, sq_magnitude_variables, sq_phase_variables, disp_magnitude_variables,\n",
    "              disp_phase_variables, kerr_variables]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing quantum layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                   Constructing quantum layers\n",
    "# ===================================================================================\n",
    "\n",
    "\n",
    "# Defining input QNN layer, whose parameters are set by the outputs of the classical network\n",
    "def input_qnn_layer():\n",
    "    with tf.name_scope('inputlayer'):\n",
    "        Sgate(tf.clip_by_value(output_layer[:, 0], -sq_clip, sq_clip), output_layer[:, 1]) | q[0]\n",
    "        Sgate(tf.clip_by_value(output_layer[:, 2], -sq_clip, sq_clip), output_layer[:, 3]) | q[1]\n",
    "\n",
    "        BSgate(output_layer[:, 4], output_layer[:, 5]) | (q[0], q[1])\n",
    "\n",
    "        Rgate(output_layer[:, 6]) | q[0]\n",
    "        Rgate(output_layer[:, 7]) | q[1]\n",
    "\n",
    "        Dgate(tf.clip_by_value(output_layer[:, 8], -disp_clip, disp_clip), output_layer[:, 9]) \\\n",
    "        | q[0]\n",
    "        Dgate(tf.clip_by_value(output_layer[:, 10], -disp_clip, disp_clip), output_layer[:, 11]) \\\n",
    "        | q[1]\n",
    "\n",
    "        Kgate(tf.clip_by_value(output_layer[:, 12], -kerr_clip, kerr_clip)) | q[0]\n",
    "        Kgate(tf.clip_by_value(output_layer[:, 13], -kerr_clip, kerr_clip)) | q[1]\n",
    "\n",
    "\n",
    "# Defining standard QNN layers\n",
    "def qnn_layer(layer_number):\n",
    "    with tf.name_scope('layer_{}'.format(layer_number)):\n",
    "        BSgate(bs_variables[layer_number, 0, 0, 0], bs_variables[layer_number, 0, 0, 1]) \\\n",
    "        | (q[0], q[1])\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Rgate(phase_variables[layer_number, i, 0]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Sgate(tf.clip_by_value(sq_magnitude_variables[layer_number, i], -sq_clip, sq_clip),\n",
    "                  sq_phase_variables[layer_number, i]) | q[i]\n",
    "\n",
    "        BSgate(bs_variables[layer_number, 0, 1, 0], bs_variables[layer_number, 0, 1, 1]) \\\n",
    "        | (q[0], q[1])\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Rgate(phase_variables[layer_number, i, 1]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Dgate(tf.clip_by_value(disp_magnitude_variables[layer_number, i], -disp_clip,\n",
    "                                   disp_clip), disp_phase_variables[layer_number, i]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Kgate(tf.clip_by_value(kerr_variables[layer_number, i], -kerr_clip, kerr_clip)) | q[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining QNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                   Defining QNN\n",
    "# ===================================================================================\n",
    "\n",
    "# construct the two-mode Strawberry Fields program\n",
    "prog = sf.Program(mode_number)\n",
    "\n",
    "# construct the circuit\n",
    "with prog.context as q:\n",
    "    input_qnn_layer()\n",
    "\n",
    "    for i in range(depth):\n",
    "        qnn_layer(i)\n",
    "\n",
    "# create an engine\n",
    "eng = sf.Engine('tf', backend_options={\"cutoff_dim\": cutoff, \"batch_size\": batch_size})\n",
    "\n",
    "# run the engine (in batch mode)\n",
    "state = eng.run(prog, run_options={\"eval\": False}).state\n",
    "# extract the state\n",
    "ket = state.ket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cost:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================================================================================\n",
    "#                                   Setting up cost function\n",
    "# ===================================================================================\n",
    "\n",
    "# Classifications for whole batch: rows act as data points in the batch and columns\n",
    "# are the one-hot classifications\n",
    "\n",
    "func_to_minimise = 0\n",
    "classification = tf.placeholder(shape=[batch_size, 2], dtype=tf.int32)\n",
    "\n",
    "# Building up the function to minimize by looping through batch\n",
    "for i in range(batch_size):\n",
    "    # Probabilities corresponding to a single photon in either mode\n",
    "    prob = tf.abs(ket[i, classification[i, 0], classification[i, 1]]) ** 2\n",
    "    # These probabilities should be optimised to 1\n",
    "    func_to_minimise += (1.0 / batch_size) * (prob-1) ** 2\n",
    "\n",
    "# Defining the cost function\n",
    "cost_func = func_to_minimise\n",
    "tf.summary.scalar('Cost', cost_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 30 11:50:14 2019\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 30 11:50:37 2019\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15KW|?'\n",
      "iteration:  1  cost:  0.985707\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  2  cost:  0.9935115\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  3  cost:  0.9852517\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  4  cost:  0.9743221\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  5  cost:  0.9988628\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  6  cost:  0.99048823\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  7  cost:  0.99581426\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  8  cost:  0.99553406\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  9  cost:  0.99431634\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  10  cost:  0.9999333\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  11  cost:  0.9957647\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  12  cost:  0.99277896\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  13  cost:  0.98489183\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  14  cost:  0.9937035\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  15  cost:  0.98810375\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  16  cost:  0.9978293\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  17  cost:  0.9969614\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  18  cost:  0.9750408\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  19  cost:  0.99066234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  20  cost:  0.9999826\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  21  cost:  0.9899831\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  22  cost:  0.98459965\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  23  cost:  0.9898126\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  24  cost:  0.994575\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  25  cost:  0.9976233\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  26  cost:  0.9914801\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  27  cost:  0.9850234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  28  cost:  0.9933664\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  29  cost:  0.9975912\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  30  cost:  0.9842934\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  31  cost:  0.99255794\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  32  cost:  0.97414947\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  33  cost:  0.9820815\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  34  cost:  0.9867726\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  35  cost:  0.9969693\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  36  cost:  0.98341554\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  37  cost:  0.9972772\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  38  cost:  0.9836362\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  39  cost:  0.99166405\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  40  cost:  0.9977277\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  41  cost:  0.99654925\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  42  cost:  0.9839115\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  43  cost:  0.9951234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  44  cost:  0.99593866\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  45  cost:  0.97494775\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  46  cost:  0.9866219\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  47  cost:  0.9902777\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  48  cost:  0.98633826\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  49  cost:  0.9818598\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  50  cost:  0.994345\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  51  cost:  0.9708217\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  52  cost:  0.9851212\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  53  cost:  0.9844477\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  54  cost:  0.9817485\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  55  cost:  0.9850021\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  56  cost:  0.99075973\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  57  cost:  0.99401027\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  58  cost:  0.9809053\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  59  cost:  0.9645698\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  60  cost:  0.9794075\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  61  cost:  0.9861257\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  62  cost:  0.9813481\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  63  cost:  0.97799736\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  64  cost:  0.99227434\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  65  cost:  0.9803902\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  66  cost:  0.9753985\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  67  cost:  0.9989441\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  68  cost:  0.9749212\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  69  cost:  0.9903995\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  70  cost:  0.9998321\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  71  cost:  0.99258786\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  72  cost:  0.9804635\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  73  cost:  0.9874137\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  74  cost:  0.96653783\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  75  cost:  0.9809183\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  76  cost:  0.99372905\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  77  cost:  0.9711073\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  78  cost:  0.97100854\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  79  cost:  0.96516526\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  80  cost:  0.9772017\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  81  cost:  0.98630655\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  82  cost:  0.98260707\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  83  cost:  0.9941137\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  84  cost:  0.98709303\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  85  cost:  0.9999091\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  86  cost:  0.9998615\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  87  cost:  0.975918\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  88  cost:  0.9998237\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  89  cost:  0.9999973\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  90  cost:  0.98405266\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  91  cost:  0.97766995\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  92  cost:  0.97220886\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  93  cost:  0.9662556\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  94  cost:  0.98624915\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  95  cost:  0.97016335\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  96  cost:  0.99403495\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  97  cost:  0.99761343\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  98  cost:  0.96103233\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  99  cost:  0.99013895\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  100  cost:  0.9952399\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15\\xaf5\\x7f?'\n",
      "iteration:  101  cost:  0.9969129\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  102  cost:  0.981318\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  103  cost:  0.9791581\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  104  cost:  0.981891\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  105  cost:  0.9651714\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  106  cost:  0.98219573\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  107  cost:  0.9675589\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  108  cost:  0.95556456\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  109  cost:  0.99458724\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  110  cost:  0.9852962\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  111  cost:  0.98874146\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  112  cost:  0.97520167\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  113  cost:  0.98259646\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  114  cost:  0.9935869\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  115  cost:  0.9780497\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  116  cost:  0.9753325\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  117  cost:  0.9654169\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  118  cost:  0.9914068\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  119  cost:  0.9778025\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  120  cost:  0.97948945\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  121  cost:  0.9570055\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  122  cost:  0.94329613\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  123  cost:  0.9572291\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  124  cost:  0.9639725\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  125  cost:  0.9978491\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  126  cost:  0.99112606\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  127  cost:  0.9550526\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  128  cost:  0.9891844\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  129  cost:  0.9775826\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  130  cost:  0.9925173\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  131  cost:  0.9769833\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  132  cost:  0.99517846\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  133  cost:  0.98122257\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  134  cost:  0.9718185\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  135  cost:  0.97229755\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  136  cost:  0.9730764\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  137  cost:  0.97387666\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  138  cost:  0.9644145\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  139  cost:  0.9679572\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  140  cost:  0.9847544\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  141  cost:  0.9923351\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  142  cost:  0.97330326\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  143  cost:  0.965307\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  144  cost:  0.97679275\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  145  cost:  0.95948416\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  146  cost:  0.9636009\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  147  cost:  0.96707433\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  148  cost:  0.9677955\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  149  cost:  0.9903743\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  150  cost:  0.9942243\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  151  cost:  0.9798562\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  152  cost:  0.96108866\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  153  cost:  0.96577096\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  154  cost:  0.9904966\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  155  cost:  0.9717286\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  156  cost:  0.98172253\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  157  cost:  0.98534536\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  158  cost:  0.9882113\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  159  cost:  0.97605073\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  160  cost:  0.97064036\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  161  cost:  0.9756602\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  162  cost:  0.9744204\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  163  cost:  0.9708694\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  164  cost:  0.97982836\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  165  cost:  0.9809171\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  166  cost:  0.99452734\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  167  cost:  0.98705375\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  168  cost:  0.97657704\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  169  cost:  0.9892895\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  170  cost:  0.9890531\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  171  cost:  0.9754499\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  172  cost:  0.98954004\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  173  cost:  0.93613356\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  174  cost:  0.9633239\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  175  cost:  0.98473287\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  176  cost:  0.9975337\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  177  cost:  0.9765503\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  178  cost:  0.96909773\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  179  cost:  0.97917295\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  180  cost:  0.9622959\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  181  cost:  0.9949372\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  182  cost:  0.977216\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  183  cost:  0.9520875\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  184  cost:  0.9669162\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  185  cost:  0.98888695\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  186  cost:  0.9957736\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  187  cost:  0.9750554\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  188  cost:  0.966333\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  189  cost:  0.9826236\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  190  cost:  0.99489903\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  191  cost:  0.96383107\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  192  cost:  0.98649687\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  193  cost:  0.9792729\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  194  cost:  0.9560362\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  195  cost:  0.9623441\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  196  cost:  0.9785645\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  197  cost:  0.9834485\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  198  cost:  0.99596757\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  199  cost:  0.9696021\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  200  cost:  0.9745353\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x156;{?'\n",
      "iteration:  201  cost:  0.98137224\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  202  cost:  0.9995368\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  203  cost:  0.9468855\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  204  cost:  0.9731086\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  205  cost:  0.98931205\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  206  cost:  0.9661694\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  207  cost:  0.9311997\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  208  cost:  0.9747225\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  209  cost:  0.9562276\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  210  cost:  0.97601044\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  211  cost:  0.9818678\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  212  cost:  0.9605316\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  213  cost:  0.96932405\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  214  cost:  0.97170204\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  215  cost:  0.9879672\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  216  cost:  0.98015314\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  217  cost:  0.98669565\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  218  cost:  0.9833553\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  219  cost:  0.9783631\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  220  cost:  0.9132976\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  221  cost:  0.97007173\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  222  cost:  0.9585945\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  223  cost:  0.9714042\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  224  cost:  0.95168626\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  225  cost:  0.981477\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  226  cost:  0.94889206\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  227  cost:  0.9876252\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  228  cost:  0.9492252\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  229  cost:  0.9743072\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  230  cost:  0.98923063\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  231  cost:  0.9933335\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  232  cost:  0.98746777\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  233  cost:  0.9746673\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  234  cost:  0.97960687\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  235  cost:  0.93776035\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  236  cost:  0.9887092\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  237  cost:  0.962954\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  238  cost:  0.97736657\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  239  cost:  0.9582725\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  240  cost:  0.9421651\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  241  cost:  0.9645304\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  242  cost:  0.94807893\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  243  cost:  0.9935876\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  244  cost:  0.9445554\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  245  cost:  0.97294706\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  246  cost:  0.96198064\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  247  cost:  0.9868068\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  248  cost:  0.9622139\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  249  cost:  0.96146154\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  250  cost:  0.9556209\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  251  cost:  0.9701959\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  252  cost:  0.997731\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  253  cost:  0.9571681\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  254  cost:  0.9623437\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  255  cost:  0.9680466\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  256  cost:  0.98330545\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  257  cost:  0.97335774\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  258  cost:  0.94546115\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  259  cost:  0.98743653\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  260  cost:  0.9564021\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  261  cost:  0.976811\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  262  cost:  0.9613712\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  263  cost:  0.9557064\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  264  cost:  0.98566157\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  265  cost:  0.96477574\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  266  cost:  0.9505458\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  267  cost:  0.9690121\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  268  cost:  0.979673\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  269  cost:  0.9578425\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  270  cost:  0.9516051\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  271  cost:  0.92625076\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  272  cost:  0.9535514\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  273  cost:  0.961656\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  274  cost:  0.9388786\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  275  cost:  0.9627049\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  276  cost:  0.9447557\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  277  cost:  0.97828734\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  278  cost:  0.95372\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  279  cost:  0.9577429\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  280  cost:  0.92920417\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  281  cost:  0.9846648\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  282  cost:  0.96810484\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  283  cost:  0.9585713\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  284  cost:  0.9742431\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  285  cost:  0.94523036\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  286  cost:  0.99160856\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  287  cost:  0.96211404\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  288  cost:  0.95871824\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  289  cost:  0.9834686\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  290  cost:  0.9820037\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  291  cost:  0.9647895\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  292  cost:  0.98252374\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  293  cost:  0.9667971\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  294  cost:  0.9612455\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  295  cost:  0.98086977\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  296  cost:  0.96416426\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  297  cost:  0.9230255\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  298  cost:  0.9823315\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  299  cost:  0.98455137\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  300  cost:  0.9722164\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15\\xa9[z?'\n",
      "iteration:  301  cost:  0.9779611\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  302  cost:  0.9605857\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  303  cost:  0.9859234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  304  cost:  0.98683995\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  305  cost:  0.97878075\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  306  cost:  0.9776432\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  307  cost:  0.95559365\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  308  cost:  0.9891684\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  309  cost:  0.98312134\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  310  cost:  0.9518983\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  311  cost:  0.954859\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  312  cost:  0.983764\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  313  cost:  0.9589983\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  314  cost:  0.98026234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  315  cost:  0.9937102\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  316  cost:  0.9709128\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  317  cost:  0.97592205\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  318  cost:  0.96538967\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  319  cost:  0.99353826\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  320  cost:  0.9808595\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  321  cost:  0.97793114\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  322  cost:  0.95502126\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  323  cost:  0.98181975\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  324  cost:  0.95939803\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  325  cost:  0.9822422\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  326  cost:  0.9803923\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  327  cost:  0.95160604\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  328  cost:  0.9753369\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  329  cost:  0.98480195\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  330  cost:  0.9940234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  331  cost:  0.985796\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  332  cost:  0.99736804\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  333  cost:  0.97732204\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  334  cost:  0.9521541\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  335  cost:  0.9591876\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  336  cost:  0.9998969\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  337  cost:  0.9700589\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  338  cost:  0.9771159\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  339  cost:  0.9863078\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  340  cost:  0.97991884\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  341  cost:  0.96578074\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  342  cost:  0.98237157\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  343  cost:  0.987124\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  344  cost:  0.9978811\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  345  cost:  0.9467792\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  346  cost:  0.95082456\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  347  cost:  0.9824507\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  348  cost:  0.99580723\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  349  cost:  0.96516234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  350  cost:  0.9939929\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  351  cost:  0.9801798\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  352  cost:  0.9533278\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  353  cost:  0.9554683\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  354  cost:  0.9687491\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  355  cost:  0.97281826\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  356  cost:  0.93811274\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  357  cost:  0.9814434\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  358  cost:  0.99700314\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  359  cost:  0.9485032\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  360  cost:  0.9326845\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  361  cost:  0.97623426\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  362  cost:  0.93202776\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  363  cost:  0.9328674\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  364  cost:  0.96739787\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  365  cost:  0.99040866\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  366  cost:  0.9590611\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  367  cost:  0.98260444\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  368  cost:  0.9330194\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  369  cost:  0.9917411\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  370  cost:  0.9443324\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  371  cost:  0.93264544\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  372  cost:  0.9551676\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  373  cost:  0.94861865\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  374  cost:  0.90061665\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  375  cost:  0.9862157\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  376  cost:  0.94557816\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  377  cost:  0.9925992\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  378  cost:  0.9773411\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  379  cost:  0.9883683\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  380  cost:  0.97673535\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  381  cost:  0.986233\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  382  cost:  0.9797037\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  383  cost:  0.96966875\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  384  cost:  0.9731919\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  385  cost:  0.98410624\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  386  cost:  0.9972228\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  387  cost:  0.9993554\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  388  cost:  0.9975899\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  389  cost:  0.96217257\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  390  cost:  0.9721677\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  391  cost:  0.9722801\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  392  cost:  0.97081643\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  393  cost:  0.9819199\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  394  cost:  0.9657903\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  395  cost:  0.9743355\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  396  cost:  0.95276767\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  397  cost:  0.9502778\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  398  cost:  0.9906088\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  399  cost:  0.94157976\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  400  cost:  0.9593411\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15\\xaa\\xc1r?'\n",
      "iteration:  401  cost:  0.9482676\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  402  cost:  0.970212\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  403  cost:  0.9506235\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  404  cost:  0.9618644\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  405  cost:  0.9573283\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  406  cost:  0.9809092\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  407  cost:  0.9514456\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  408  cost:  0.9514558\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  409  cost:  0.96690893\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  410  cost:  0.95550984\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  411  cost:  0.9600244\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  412  cost:  0.98534214\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  413  cost:  0.99090433\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  414  cost:  0.93731034\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  415  cost:  0.94783044\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  416  cost:  0.9888328\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  417  cost:  0.97107697\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  418  cost:  0.9463528\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  419  cost:  0.9626935\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  420  cost:  0.9824414\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  421  cost:  0.973625\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  422  cost:  0.9601152\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  423  cost:  0.94541794\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  424  cost:  0.9443239\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  425  cost:  0.95178294\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  426  cost:  0.9215147\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  427  cost:  0.97418106\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  428  cost:  0.939173\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  429  cost:  0.9617345\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  430  cost:  0.9575116\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  431  cost:  0.9684245\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  432  cost:  0.9707343\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  433  cost:  0.9831596\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  434  cost:  0.9765212\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  435  cost:  0.97511715\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  436  cost:  0.95971984\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  437  cost:  0.9638395\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  438  cost:  0.9627193\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  439  cost:  0.9406952\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  440  cost:  0.96621513\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  441  cost:  0.97662854\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  442  cost:  0.909127\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  443  cost:  0.95012355\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  444  cost:  0.9690762\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  445  cost:  0.96746695\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  446  cost:  0.9933102\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  447  cost:  0.9530265\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  448  cost:  0.9465917\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  449  cost:  0.96235645\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  450  cost:  0.9560499\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  451  cost:  0.9548337\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  452  cost:  0.9691348\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  453  cost:  0.9690912\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  454  cost:  0.96104926\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  455  cost:  0.9420056\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  456  cost:  0.9821694\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  457  cost:  0.9797713\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  458  cost:  0.9882207\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  459  cost:  0.9815921\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  460  cost:  0.96029234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  461  cost:  0.9684313\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  462  cost:  0.97445685\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  463  cost:  0.9892242\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  464  cost:  0.99217385\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  465  cost:  0.9769823\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  466  cost:  0.9725801\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  467  cost:  0.9438663\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  468  cost:  0.98377097\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  469  cost:  0.96541816\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  470  cost:  0.99244654\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  471  cost:  0.96951205\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  472  cost:  0.97519433\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  473  cost:  0.9864723\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  474  cost:  0.94437623\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  475  cost:  0.9582462\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  476  cost:  0.9919034\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  477  cost:  0.9670243\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  478  cost:  0.99266493\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  479  cost:  0.97394335\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  480  cost:  0.94797075\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  481  cost:  0.9744929\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  482  cost:  0.9876938\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  483  cost:  0.95532084\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  484  cost:  0.9759087\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  485  cost:  0.9310212\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  486  cost:  0.9708648\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  487  cost:  0.9706686\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  488  cost:  0.9059011\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  489  cost:  0.93567294\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  490  cost:  0.97280794\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  491  cost:  0.9406979\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  492  cost:  0.9909344\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  493  cost:  0.97670245\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  494  cost:  0.927535\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  495  cost:  0.969374\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  496  cost:  0.94195336\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  497  cost:  0.96620196\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  498  cost:  0.9671295\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  499  cost:  0.98092294\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  500  cost:  0.97082853\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15yh{?'\n",
      "iteration:  501  cost:  0.9820629\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  502  cost:  0.9235453\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  503  cost:  0.9514815\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  504  cost:  0.96410435\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  505  cost:  0.9464459\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  506  cost:  0.9432859\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  507  cost:  0.97328806\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  508  cost:  0.89834046\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  509  cost:  0.9637339\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  510  cost:  0.95406705\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  511  cost:  0.99120253\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  512  cost:  0.9463362\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  513  cost:  0.9816919\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  514  cost:  0.9802245\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  515  cost:  0.93917066\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  516  cost:  0.9776484\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  517  cost:  0.9922714\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  518  cost:  0.95115316\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  519  cost:  0.9531319\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  520  cost:  0.98147327\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  521  cost:  0.9465597\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  522  cost:  0.95939124\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  523  cost:  0.96447086\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  524  cost:  0.94272447\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  525  cost:  0.9383694\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  526  cost:  0.9690583\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  527  cost:  0.9646615\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  528  cost:  0.96529377\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  529  cost:  0.97380555\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  530  cost:  0.92260087\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  531  cost:  0.95703316\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  532  cost:  0.9494339\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  533  cost:  0.9503887\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  534  cost:  0.93463373\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  535  cost:  0.93368626\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  536  cost:  0.9749205\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  537  cost:  0.9269987\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  538  cost:  0.9568275\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  539  cost:  0.954987\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  540  cost:  0.99315023\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  541  cost:  0.9759748\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  542  cost:  0.9553219\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  543  cost:  0.95357335\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  544  cost:  0.9609408\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  545  cost:  0.941696\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  546  cost:  0.9622937\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  547  cost:  0.9336879\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  548  cost:  0.9510448\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  549  cost:  0.91420364\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  550  cost:  0.9195113\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  551  cost:  0.9769325\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  552  cost:  0.9595721\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  553  cost:  0.9889232\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  554  cost:  0.9590789\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  555  cost:  0.9642058\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  556  cost:  0.945929\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  557  cost:  0.9661819\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  558  cost:  0.9256827\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  559  cost:  0.9720628\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  560  cost:  0.9332979\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  561  cost:  0.9677032\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  562  cost:  0.9466276\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  563  cost:  0.95108783\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  564  cost:  0.9810628\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  565  cost:  0.98283505\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  566  cost:  0.94572026\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  567  cost:  0.93718153\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  568  cost:  0.96891916\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  569  cost:  0.97598636\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  570  cost:  0.9518979\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  571  cost:  0.90964484\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  572  cost:  0.97357845\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  573  cost:  0.9870568\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  574  cost:  0.94160295\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  575  cost:  0.9147635\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  576  cost:  0.9412271\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  577  cost:  0.96383286\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  578  cost:  0.9758142\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  579  cost:  0.98462653\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  580  cost:  0.91963166\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  581  cost:  0.95898557\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  582  cost:  0.96891975\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  583  cost:  0.9632838\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  584  cost:  0.9569013\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  585  cost:  0.93647844\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  586  cost:  0.9541821\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  587  cost:  0.9632565\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  588  cost:  0.94428265\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  589  cost:  0.9612332\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  590  cost:  0.9368961\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  591  cost:  0.93407875\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  592  cost:  0.94472206\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  593  cost:  0.9536021\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  594  cost:  0.9321461\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  595  cost:  0.9488019\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  596  cost:  0.9443744\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  597  cost:  0.9560338\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  598  cost:  0.9124279\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  599  cost:  0.937316\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  600  cost:  0.9333149\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15\\xe1\\x81v?'\n",
      "iteration:  601  cost:  0.9629193\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  602  cost:  0.92748195\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  603  cost:  0.96774983\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  604  cost:  0.97439337\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  605  cost:  0.9689887\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  606  cost:  0.9688742\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  607  cost:  0.9329584\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  608  cost:  0.9324987\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  609  cost:  0.948937\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  610  cost:  0.9569953\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  611  cost:  0.93422425\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  612  cost:  0.94095796\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  613  cost:  0.97781754\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  614  cost:  0.9485388\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  615  cost:  0.9680091\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  616  cost:  0.94082505\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  617  cost:  0.9390315\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  618  cost:  0.91827613\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  619  cost:  0.95021236\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  620  cost:  0.98384154\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  621  cost:  0.9154561\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  622  cost:  0.9663836\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  623  cost:  0.9119079\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  624  cost:  0.9674862\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  625  cost:  0.972093\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  626  cost:  0.9713989\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  627  cost:  0.95673317\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  628  cost:  0.930082\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  629  cost:  0.9581016\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  630  cost:  0.9836106\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  631  cost:  0.962109\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  632  cost:  0.9348247\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  633  cost:  0.9666574\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  634  cost:  0.9028921\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  635  cost:  0.96914095\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  636  cost:  0.91045386\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  637  cost:  0.9630029\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  638  cost:  0.9359731\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  639  cost:  0.96363026\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  640  cost:  0.9503862\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  641  cost:  0.90374434\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  642  cost:  0.9000687\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  643  cost:  0.92604196\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  644  cost:  0.9645295\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  645  cost:  0.92310554\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  646  cost:  0.91890204\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  647  cost:  0.9769046\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  648  cost:  0.919582\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  649  cost:  0.9077045\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  650  cost:  0.9003848\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  651  cost:  0.9019419\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  652  cost:  0.9436041\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  653  cost:  0.9844974\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  654  cost:  0.9004638\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  655  cost:  0.9221615\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  656  cost:  0.96619105\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  657  cost:  0.95106745\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  658  cost:  0.9339065\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  659  cost:  0.95354325\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  660  cost:  0.90659815\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  661  cost:  0.9023202\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  662  cost:  0.9588366\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  663  cost:  0.95949286\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  664  cost:  0.951299\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  665  cost:  0.8953403\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  666  cost:  0.95314527\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  667  cost:  0.92843795\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  668  cost:  0.9497935\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  669  cost:  0.9599821\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  670  cost:  0.91520196\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  671  cost:  0.9287727\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  672  cost:  0.95487213\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  673  cost:  0.9349328\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  674  cost:  0.9470625\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  675  cost:  0.96159405\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  676  cost:  0.939526\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  677  cost:  0.9199638\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  678  cost:  0.95108545\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  679  cost:  0.9411211\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  680  cost:  0.9251103\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  681  cost:  0.9660619\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  682  cost:  0.8962008\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  683  cost:  0.9264342\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  684  cost:  0.9168843\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  685  cost:  0.94013757\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  686  cost:  0.94926614\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  687  cost:  0.94730705\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  688  cost:  0.93678564\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  689  cost:  0.94521517\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  690  cost:  0.96558255\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  691  cost:  0.92952245\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  692  cost:  0.92114294\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  693  cost:  0.90421975\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  694  cost:  0.8946507\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  695  cost:  0.9250611\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  696  cost:  0.90829533\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  697  cost:  0.92208135\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  698  cost:  0.93874264\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  699  cost:  0.9714759\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  700  cost:  0.9331534\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15\\xbe\\xceo?'\n",
      "iteration:  701  cost:  0.9367484\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  702  cost:  0.94950944\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  703  cost:  0.96579146\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  704  cost:  0.94182444\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  705  cost:  0.9303515\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  706  cost:  0.9767262\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  707  cost:  0.9317805\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  708  cost:  0.9311815\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  709  cost:  0.9271093\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  710  cost:  0.91022426\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  711  cost:  0.94891655\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  712  cost:  0.8934835\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  713  cost:  0.8872327\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  714  cost:  0.94256604\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  715  cost:  0.91445214\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  716  cost:  0.92131823\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  717  cost:  0.9071785\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  718  cost:  0.89979845\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  719  cost:  0.92986554\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  720  cost:  0.96881336\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  721  cost:  0.96085453\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  722  cost:  0.95695007\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  723  cost:  0.934617\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  724  cost:  0.9440895\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  725  cost:  0.8975288\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  726  cost:  0.94229233\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  727  cost:  0.93469864\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  728  cost:  0.8956542\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  729  cost:  0.9697705\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  730  cost:  0.92713594\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  731  cost:  0.8771234\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  732  cost:  0.88686484\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  733  cost:  0.8935139\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  734  cost:  0.9353127\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  735  cost:  0.94969296\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  736  cost:  0.93271524\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  737  cost:  0.93786144\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  738  cost:  0.9114226\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  739  cost:  0.9550315\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  740  cost:  0.95145804\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  741  cost:  0.92319554\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  742  cost:  0.92065203\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  743  cost:  0.93836266\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  744  cost:  0.9355832\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  745  cost:  0.95926386\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  746  cost:  0.9304923\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  747  cost:  0.95678985\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  748  cost:  0.93954724\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  749  cost:  0.9787013\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  750  cost:  0.94955957\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  751  cost:  0.9525942\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  752  cost:  0.92719615\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  753  cost:  0.943428\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  754  cost:  0.9201\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  755  cost:  0.93520916\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  756  cost:  0.9647001\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  757  cost:  0.93964803\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  758  cost:  0.9155341\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  759  cost:  0.9214104\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  760  cost:  0.9260204\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  761  cost:  0.9150852\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  762  cost:  0.90354323\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  763  cost:  0.9392215\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  764  cost:  0.94346476\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  765  cost:  0.8808033\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  766  cost:  0.91267776\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  767  cost:  0.9301001\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  768  cost:  0.95871997\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  769  cost:  0.92487466\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  770  cost:  0.9145273\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  771  cost:  0.8871398\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  772  cost:  0.8998751\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  773  cost:  0.90394676\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  774  cost:  0.9197629\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  775  cost:  0.946761\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  776  cost:  0.93704414\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  777  cost:  0.98160195\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  778  cost:  0.9191762\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  779  cost:  0.9517068\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  780  cost:  0.8962478\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  781  cost:  0.912081\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  782  cost:  0.91380453\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  783  cost:  0.9667871\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  784  cost:  0.8904037\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  785  cost:  0.9047171\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  786  cost:  0.9229209\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  787  cost:  0.934264\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  788  cost:  0.89697695\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  789  cost:  0.9884876\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  790  cost:  0.9346393\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  791  cost:  0.9680669\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  792  cost:  0.9313702\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  793  cost:  0.91497296\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  794  cost:  0.9411014\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  795  cost:  0.94771284\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  796  cost:  0.9278572\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  797  cost:  0.94288456\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  798  cost:  0.9117048\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  799  cost:  0.88560545\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  800  cost:  0.9139382\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15\\xbc\\x13m?'\n",
      "iteration:  801  cost:  0.9260824\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  802  cost:  0.8951485\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  803  cost:  0.9054497\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  804  cost:  0.9490172\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  805  cost:  0.9459176\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  806  cost:  0.937124\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  807  cost:  0.9720837\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  808  cost:  0.9184933\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  809  cost:  0.93355846\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  810  cost:  0.92441285\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  811  cost:  0.9172782\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  812  cost:  0.88459355\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  813  cost:  0.9448782\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  814  cost:  0.8838233\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  815  cost:  0.89955014\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  816  cost:  0.912909\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  817  cost:  0.92510337\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  818  cost:  0.9237458\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  819  cost:  0.9157031\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  820  cost:  0.94660825\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  821  cost:  0.94533634\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  822  cost:  0.93704295\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  823  cost:  0.9194011\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  824  cost:  0.9324783\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  825  cost:  0.89606774\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  826  cost:  0.89623076\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  827  cost:  0.9401362\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  828  cost:  0.9235625\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  829  cost:  0.9383497\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  830  cost:  0.87179387\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  831  cost:  0.8866245\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  832  cost:  0.9213065\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  833  cost:  0.9317766\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  834  cost:  0.9585448\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  835  cost:  0.94849336\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  836  cost:  0.92362726\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  837  cost:  0.92856365\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  838  cost:  0.8955018\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  839  cost:  0.9304282\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  840  cost:  0.95616263\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  841  cost:  0.8944389\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  842  cost:  0.8632409\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  843  cost:  0.8044485\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  844  cost:  0.90891767\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  845  cost:  0.8929307\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  846  cost:  0.94820684\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  847  cost:  0.9416957\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  848  cost:  0.904057\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  849  cost:  0.9261689\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  850  cost:  0.9421794\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  851  cost:  0.9336793\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  852  cost:  0.95202595\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  853  cost:  0.96502405\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  854  cost:  0.95206255\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  855  cost:  0.93418324\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  856  cost:  0.9145025\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  857  cost:  0.9617757\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  858  cost:  0.888012\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  859  cost:  0.9251857\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  860  cost:  0.95110816\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  861  cost:  0.9695742\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  862  cost:  0.9362153\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  863  cost:  0.9418898\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  864  cost:  0.948627\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  865  cost:  0.9339574\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  866  cost:  0.8938823\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  867  cost:  0.88377666\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  868  cost:  0.93450004\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  869  cost:  0.93401116\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  870  cost:  0.8910804\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  871  cost:  0.9187791\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  872  cost:  0.9182435\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  873  cost:  0.91523105\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  874  cost:  0.8710398\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  875  cost:  0.93668306\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  876  cost:  0.96791935\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  877  cost:  0.89687216\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  878  cost:  0.8837335\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  879  cost:  0.9202209\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  880  cost:  0.9432463\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  881  cost:  0.9528908\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  882  cost:  0.9183322\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  883  cost:  0.94979703\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  884  cost:  0.97526467\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  885  cost:  0.9438073\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  886  cost:  0.8826245\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  887  cost:  0.9548565\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  888  cost:  0.8952796\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  889  cost:  0.9528906\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  890  cost:  0.91076905\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  891  cost:  0.90537715\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  892  cost:  0.9727051\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  893  cost:  0.9323566\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  894  cost:  0.9573362\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  895  cost:  0.9073897\n",
      "training_run  :    None\n",
      "++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  896  cost:  0.89461243\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  897  cost:  0.9317103\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  898  cost:  0.90837365\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  899  cost:  0.92874825\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  900  cost:  0.92129695\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x15\\xd2\\xddf?'\n",
      "iteration:  901  cost:  0.9018222\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  902  cost:  0.9427431\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  903  cost:  0.9276906\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  904  cost:  0.9004612\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  905  cost:  0.9672557\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  906  cost:  0.8851419\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  907  cost:  0.89312655\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  908  cost:  0.9154116\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  909  cost:  0.9004212\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  910  cost:  0.86253166\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  911  cost:  0.86051625\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  912  cost:  0.9132095\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  913  cost:  0.9094115\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  914  cost:  0.96311074\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  915  cost:  0.91454613\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  916  cost:  0.9260261\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  917  cost:  0.85131764\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  918  cost:  0.9180682\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  919  cost:  0.9404183\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  920  cost:  0.9614858\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  921  cost:  0.9211105\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  922  cost:  0.9246229\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  923  cost:  0.8681677\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  924  cost:  0.906641\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  925  cost:  0.886389\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  926  cost:  0.93291414\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  927  cost:  0.9176629\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  928  cost:  0.90911096\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  929  cost:  0.9015411\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  930  cost:  0.9139926\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  931  cost:  0.9273611\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  932  cost:  0.8761413\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  933  cost:  0.9505524\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  934  cost:  0.94357467\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  935  cost:  0.94104886\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  936  cost:  0.9316651\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  937  cost:  0.91370076\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  938  cost:  0.91661453\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  939  cost:  0.8946933\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  940  cost:  0.93359905\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  941  cost:  0.9217943\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  942  cost:  0.94832814\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  943  cost:  0.92558897\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  944  cost:  0.8987533\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  945  cost:  0.91433305\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  946  cost:  0.9348741\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  947  cost:  0.913587\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  948  cost:  0.8424929\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  949  cost:  0.89823776\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  950  cost:  0.9259782\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  951  cost:  0.9670373\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  952  cost:  0.9371789\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  953  cost:  0.9402705\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  954  cost:  0.91109437\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  955  cost:  0.9126492\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  956  cost:  0.88120365\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  957  cost:  0.9351355\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  958  cost:  0.9049531\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  959  cost:  0.9150331\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  960  cost:  0.9194404\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  961  cost:  0.93198836\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  962  cost:  0.88180274\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  963  cost:  0.9146663\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  964  cost:  0.9693994\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  965  cost:  0.9516091\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  966  cost:  0.89514494\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  967  cost:  0.8810205\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  968  cost:  0.9045198\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  969  cost:  0.88219285\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  970  cost:  0.91186106\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  971  cost:  0.86964756\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  972  cost:  0.9077873\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  973  cost:  0.9481109\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  974  cost:  0.88658315\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  975  cost:  0.8810279\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  976  cost:  0.91587335\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  977  cost:  0.9063853\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  978  cost:  0.94019455\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  979  cost:  0.9256133\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  980  cost:  0.8891392\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  981  cost:  0.9315106\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  982  cost:  0.9268142\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  983  cost:  0.9470928\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  984  cost:  0.9335081\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  985  cost:  0.8514306\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  986  cost:  0.9265192\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  987  cost:  0.89633024\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  988  cost:  0.88784313\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  989  cost:  0.901833\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  990  cost:  0.9500575\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  991  cost:  0.90691966\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  992  cost:  0.92267907\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  993  cost:  0.9078582\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  994  cost:  0.8931206\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  995  cost:  0.9427609\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  996  cost:  0.90213376\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  997  cost:  0.914841\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  998  cost:  0.87900406\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  999  cost:  0.9688562\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "iteration:  1000  cost:  0.86242723\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "summary :   b'\\n\\x0b\\n\\x04Cost\\x150\\x91_?'\n",
      "iteration:  1001  cost:  0.87330914\n",
      "training_run  :    None\n",
      "++++++++++++++\n",
      "--- 2006.8114910125732 seconds ---\n",
      "Sat Nov 30 12:24:04 2019\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================================\n",
    "#                                   Training\n",
    "# ===================================================================================\n",
    "\n",
    "# We choose the Adam optimizer\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=0.001 , beta1=0.05)\n",
    "training = optimiser.minimize(cost_func)\n",
    "\n",
    "# Saver/Loader for outputting model\n",
    "saver = tf.train.Saver(parameters)\n",
    "data\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Load previous model if non-zero ckpt_val is specified\n",
    "if ckpt_val != 0:\n",
    "    saver.restore(session, checkpoint_string + 'sess.ckpt-' + str(ckpt_val))\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = tf.summary.FileWriter(board_string)\n",
    "merge = tf.summary.merge_all()\n",
    "\n",
    "counter = ckpt_val\n",
    "\n",
    "# Tracks optimum value found (set high so first iteration encodes value)\n",
    "opt_val = 1e20\n",
    "# Batch number in which optimum value occurs\n",
    "opt_position = 0\n",
    "# Flag to detect if new optimum occured in last batch\n",
    "new_opt = False\n",
    "\n",
    "#TIME\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(time.asctime( time.localtime(time.time()) ))\n",
    "\n",
    "while counter <= reps:\n",
    "    # Shuffles data to create new epoch\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Splits data into batches\n",
    "    split_data = np.split(data, data_points / batch_size)\n",
    "\n",
    "    for batch in split_data:\n",
    "        print(\"++++++++++++++\")\n",
    "        if counter > reps:\n",
    "            break\n",
    "\n",
    "        # Input data (provided as principal components)\n",
    "        #data_points_principal_components = batch[:, 1:input_neurons + 1]\n",
    "        classes = batch[: , 42:]\n",
    "        batch_data = batch[: , :42]\n",
    "        # Data classes\n",
    "\n",
    "        #classes = batch[: ,42: ]\n",
    "        #print(data_points_principal_components.shape)\n",
    "        # Encoding classes into one-hot form\n",
    "        one_hot_input = np.zeros((batch_size, 2))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if classes[i] == 0:\n",
    "                # Encoded such that genuine transactions should be outputted as a photon in the first mode\n",
    "                one_hot_input[i] = [1, 0]\n",
    "            else:\n",
    "                one_hot_input[i] = [0, 1]\n",
    "\n",
    "        \n",
    "        #print(\"one hot shape : \",one_hot_input.shape)\n",
    "        #print(\"data shape : \" ,batch_data.shape)\n",
    "        \n",
    "        # Output to TensorBoard\n",
    "        if counter % tb_reps == 0:\n",
    "            [summary, training_run, func_to_minimise_run] = session.run([merge, training, func_to_minimise],\n",
    "                                                                        feed_dict={\n",
    "                                                                            input_classical_layer:\n",
    "                                                                                batch_data,\n",
    "                                                                            classification: one_hot_input\n",
    "                                                                            })\n",
    "            writer.add_summary(summary, counter)\n",
    "            print(\"summary :  \",summary)\n",
    "\n",
    "        else:\n",
    "            # Standard run of training\n",
    "            [training_run, func_to_minimise_run] = session.run([training, func_to_minimise], feed_dict={\n",
    "                input_classical_layer: batch_data, classification: one_hot_input})\n",
    "            \n",
    "        # Ensures cost function is well behaved\n",
    "        if np.isnan(func_to_minimise_run):\n",
    "            compute_grads = session.run(optimiser.compute_gradients(cost_func),\n",
    "                                        feed_dict={input_classical_layer: batch_data,\n",
    "                                                   classification: one_hot_input})\n",
    "            print(\"compute grads:  \",compute_grads)\n",
    "\n",
    "            if not os.path.exists(checkpoint_string):\n",
    "                os.makedirs(checkpoint_string)\n",
    "            # If cost function becomes NaN, output value of gradients for investigation\n",
    "            np.save(checkpoint_string + 'NaN.npy', compute_grads)\n",
    "            print('NaNs outputted - leaving at step ' + str(counter))\n",
    "            raise SystemExit\n",
    "\n",
    "        # Test to see if new optimum found in current batch\n",
    "        if func_to_minimise_run < opt_val:\n",
    "            opt_val = func_to_minimise_run\n",
    "            opt_position = counter\n",
    "            new_opt = True\n",
    "\n",
    "        # Save model every fixed number of batches, provided a new optimum value has occurred\n",
    "        if (counter % savr_reps == 0) and (i != 0) and new_opt and (not np.isnan(func_to_minimise_run)):\n",
    "            if not os.path.exists(checkpoint_string):\n",
    "                os.makedirs(checkpoint_string)\n",
    "            saver.save(session, checkpoint_string + 'sess.ckpt', global_step=counter)\n",
    "            # Saves position of optimum and corresponding value of cost function\n",
    "            np.savetxt(checkpoint_string + 'optimum.txt', [opt_position, opt_val])\n",
    "\n",
    "        counter += 1\n",
    "        print(\"iteration: \",counter , \" cost: \",func_to_minimise_run)\n",
    "        print(\"training_run  :   \",training_run)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
